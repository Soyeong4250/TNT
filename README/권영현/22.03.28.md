# 💀 22.03.28

## 오전
- frontEnd 구조 변경
- 전문가 리뷰 2차

## 🎈 전문가 리뷰 2차 22/03/28


1. 저희가 뉴스를 오전 6시 오후6시에 2번 수집하고 처리할 예정인데 이러면 사이트 운영하는 서버와 서버를 수집하고 저장하는 서버 2개로 나누어서 구동하는 것이 바람직 한지
- > 일반적인 기업에서는 사이트를 운영하는 서버, 수집하는 서버, 저장하는 서버 이렇게 3가지로 나누어서 운영한다. 
- > 일반 서버는 AWS, 수집 저장은 클러스터 서버에서 구동하면 될 것 같다.
2. 크롤링 20분 데이터 자료는 25MB정도가 나옵니다. 이러한 용량이 빅데이터라고 하는게 알맞을 지?
- > 태풍이 일어났는지 판단하는 어플 = 게시판의 글들을 활용해서 구현했었다. 지금 규모로는 빅데이터라고 보기에는 어렵지만 하둡을 사용했기 때문에 인프라 관점에서는 하둡을 사용했기 때문에 빅데이터라고 볼 수 있지만 규모만 따지면 빅데이터라고 따지기는 어렵다.
3. 특정 이슈별로 묶을 뉴스 클러스터링 과정에서 해당 기사의 유사도를 어떻게 판단해야할지?
- > 현업에서 사용하는 툴은 따로 없다. 중요도나 유사도를 판단하는 기준이 다 달라서 말하기는 애매하지만 각 단어 중요도 계산할 때 **TFIDF**(Term Frequency-Inverse Document Frequency)를 주로 사용한다.
- > 구글 언어 분석 BERT 도입해서 AI 기반학습하여 key mapping하기도 한다.
4. 빅데이터 분산 결과에 대한 정확성 판단 기준이 따로 있을까요?
- > data Validation 분야로 텍스트 부분에서는 옳게 나왔는지 기준 자체가 모호하고 어렵다는 것이 현실이다. 정확성 판단의 기준은 사람이 직접확인하거나 결과 값이 예상 범위 내에 존재하는가 판단하는 부분이 있다.
5. 우리가 수집하는 데이터 신뢰도를 측정하는 기준이 있나요? 또는 측정하는 툴이 존재하나요?
- > 평균 값이 N 이상이면 이상하다고 판단. or Hive Query 기반 툴들을 사용하는 경우가 있다.
6. 데이터 수집과 처리 시나리오를 어떻게 하는게 더 효율적이고 개연성이 있을까요? 수집 -> 키워드 가공 -> 하둡 저장 후 분산처리 or 수집 -> 로우데이터 저장 -> 하둡에 저장 후 분산처리
- > 효율적인 관점에서는 수집 -> 로우 데이터 저장 -> 키워드 분석 시나리오를 선택하는 것이 좋다. 이를 data warehouse 라고 한다.
- > 보통 후자를 선택하고 하둡을 돌릴때는 다른 하둡 에코시스템을 사용하게 될 것 ex) 루심?
- > 간단하게 구현하기 위해서는 전자의 방법을 선택하는 것이 적절할 것
7. 데이터 수집을 Python이 아닌 JAVA로 진행하는 경우가 있나요??
- > 세계적인 추세는 Python이 대세이다. Naver, Samsung, 쿠팡 등은 아직 JAVA로 진행하고 있다. 
- > JAVA는 Map Reduce 페이즈에서 사용된다. Spark 코드는 찾기 쉽지만 Map Reduce는 찾기 어려울 것이다.
  

## 기타 질문
8. 쿠팡 코딩테스트
- > 1차 리더분과 코딩을 진행한다.
- > 2차는 4시간 정도 코딩을 진행한다. 1,2교시 알고리즘 2~3문제 나머지는 1시간에 1문제 or 2문제 어려운 문제를 푼다 ex) System architecture 어떻게 프로젝트 구성할 것인지.
9. 쿠팡 실무
- > 배송을 최적화는 프로젝트를 진행하고 있습니다. 배송 기사의 노동 시간에 맞춰서 적절한 업무량을 할당하는 것 업무 환경을 다 수치화해서 pipeline을 만들어서 배송 기사들에게 제공하는 것
10. 기타
- > 스타트업에서 머무는 기업과 대기업으로 성장하는 기업의 차이는 빅데이터가 존재하는지에 따라 다르다고 생각한다. 따라서 빅데이터 프로젝트를 잘 공부하면 도움이 많이 될 것이다.

## 오후
- 크롤링, WordCount 소스 분석
- **TFIDF** 알고리즘 구현
- 크롤링, WordCount Spring 프로젝트로 옮기기
- AWS Redis 설치 및 연결 테스트 성공

## 🧐 향후 할 일
```
1. Spring 과 Redis 연결하기
2. WordCount 하둡에 돌리기
3. 크롤링한 뉴스 파일 개별 or 언론사별로 cluter 서버에 저장하기
4. hdfs 공부하기
```

