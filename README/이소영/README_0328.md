# 2022.03.28

### ✔ 오늘 할 일

- [x] 2차 전문가 리뷰 참여
- [ ] 게시판 상세페이지 띄우기



### 💬 2차 전문가 리뷰

Q. 저희가 뉴스를 오전 6시 오후6시에 2번 수집하고 처리할 예정인데 이러면 사이트 운영하는 서버와 서버를 수집하고 저장하는 서버 2개로 나누어서 구동하는 것이 바람직 한지

```markdown
- 동작상의 정답은 없다

- 일반적인 기업에서는 사이트를 운영하는 서버, 수집하는 서버, 저장하는 서버 이렇게 3가지로 나누어서 운영한다.
- 일반 서버는 AWS, 수집 저장은 클러스터 서버에서 구동하면 될 것 같다.
```



Q. 크롤링 20분 데이터 자료는 25MB정도가 나옵니다. 이러한 용량이 빅데이터라고 하는게 알맞을 지?

```markdown
- 데이터의 규모를 키우는 것을 쉽다.
- ex) 태풍이 일어났는지 판단하는 어플 = 게시판의 글들을 활용해서 구현했었다. 지금 규모로는 빅데이터라고 보기에는 어렵지만 하둡을 사용했기 때문에 인프라 관점에서는 하둡을 사용했기 때문에 빅데이터라고 볼 수 있지만 규모만 따지면 빅데이터라고 따지기는 어렵다.
```



Q. 특정 이슈별로 묶을 뉴스 클러스터링 과정에서 해당 기사의 유사도를 어떻게 판단해야할지?

```markdown
- 현업에서 사용하는 툴은 따로 없다. 중요도나 유사도를 판단하는 기준이 다 달라서 말하기는 애매하지만 각 단어 중요도 계산할 때 ***TFIDF****(Term Frequency-Inverse Document Frequency)를 주로 사용한다.

- 구글 언어 분석 BERT 도입해서 AI 기반학습하여 key mapping하기도 한다.
```



Q. 빅데이터 분산 결과에 대한 정확성 판단 기준이 따로 있을까요?

```markdown
- data Validation 분야로 텍스트 부분에서는 옳게 나왔는지 기준 자체가 모호하고 어렵다는 것이 현실이다. 정확성 판단의 기준은 사람이 직접확인하거나 결과 값이 예상 범위 내에 존재하는가 판단하는 부분이 있다.
```



Q. 우리가 수집하는 데이터 신뢰도를 측정하는 기준이 있나요? 또는 측정하는 툴이 존재하나요?

```markdown
- 평균 값이 N 이상이면 이상하다고 판단. or Hive Query 기반 툴들을 사용하는 경우가 있다.
```



Q. 데이터 수집과 처리 시나리오를 어떻게 하는게 더 효율적이고 개연성이 있을까요? 

> 1️⃣ 수집 -> 키워드 가공 -> 하둡 저장 후 분산처리 
>
> 2️⃣ 수집 -> 로우데이터 저장 -> 하둡에 저장 후 분산처리

```markdown
- 효율적인 관점에서는 2️⃣시나리오를 선택하는 것이 좋다. 데이터를 수집하고 로우데이터를 저장하는 것을 data warehouse 라고 한다.
- 보통 후자를 선택하고 하둡을 돌릴때는 다른 하둡 에코시스템을 사용하게 될 것
- 간단하게 구현하기 위해서는 전자의 방법을 선택하는 것이 적절할 것
```



Q. 데이터 수집을 Python이 아닌 JAVA로 진행하는 경우가 있나요??

```markdown
- 세계적인 추세는 Python이 대세. 네이버, 삼성, 쿠팡 등은 아직 JAVA로 진행하고 있다.

- JAVA는 Map Reduce 페이즈에서 사용된다. Spark 코드는 찾기 쉽지만 Map Reduce는 찾기 어려울 것이다.
```





### ❓ 게시판 상세페이지 띄우기

##### 문제점

👉 공지사항 글의 정보를 받아오기 전에 화면이 렌더링이 되고, 그 이후 값이 뒤늦게 들어옴

##### 시도

1️⃣ Getters에 있는 notice 활용

2️⃣ Actions로 접근

3️⃣ async, await 이용

👉 모두 실패..



### 📁 일과 후 할일

1️⃣ 공지사항 새로 만들어보기

2️⃣ 알고리즘 학습

